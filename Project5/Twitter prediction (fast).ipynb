{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popularity Prediction\n",
    "## Problem 1.1\n",
    "As a preliminary step, we calculated the following statistics to get a holistic overview of the twitter dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import os\n",
    "import tqdm #python for loop progress bar\n",
    "\n",
    "import datetime, time\n",
    "import pytz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#these values were found via wc -l in CLI\n",
    "hashtag_dict = {\n",
    "    'gohawks':188136,\n",
    "    'gopatriots':26232,\n",
    "    'nfl':259024,\n",
    "    'patriots':489713,\n",
    "    'sb49':826951,\n",
    "    'superbowl':1348767\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "def load_data(filename=None, num_tweets=0):\n",
    "    with open(filename, 'r') as file:\n",
    "        df = pd.DataFrame(index=range(num_tweets),\n",
    "                         columns=['date', 'total_tweets', 'total_retweets', 'sum_followers',\n",
    "                                 'max_followers', 'total_replies', 'total_ranking',\n",
    "                                 'total_impressions'])\n",
    "\n",
    "#total_tweets, total_retweets, sum_followers, \n",
    "#max_followers, time_of_day, toatl_replies, total_ranking, total_impressions\n",
    "        user_dict = {}\n",
    "        for i, l in tqdm.tqdm(enumerate(file), total=num_tweets):\n",
    "        \n",
    "        #pandas df has first element as index and second as col\n",
    "            tweet = json.loads(l)\n",
    "\n",
    "            #get the date for sorting\n",
    "            date = datetime.datetime.fromtimestamp(tweet['firstpost_date'])\n",
    "            df.set_value(i, 'date', date)\n",
    "            df.set_value(i, 'total_tweets', 1)\n",
    "            df.set_value(i, 'total_retweets', tweet['metrics']['citations']['total'])\n",
    "\n",
    "            #will sum and take max in post processing\n",
    "            df.set_value(i, 'sum_followers', tweet['author']['followers'])\n",
    "            df.set_value(i, 'max_followers', tweet['author']['followers'])\n",
    "            df.set_value(i, 'total_replies', tweet['metrics']['citations']['replies'])\n",
    "            df.set_value(i, 'total_ranking', tweet['metrics']['ranking_score'])\n",
    "            df.set_value(i, 'total_impressions', tweet['metrics']['impressions'])\n",
    "            \n",
    "            user_id = tweet['tweet']['user']['id']\n",
    "            user_dict[user_id] = tweet['author']['followers']\n",
    "        return df, user_dict\n",
    "\n",
    "\n",
    "   # print('#%s' % hashtag)\n",
    "   # print('\\tAvg # of tweets / hour = %.3f' % (total_tweets / total_hours))\n",
    "   # print('\\tAvg # of followers / user = %.3f' % (total_followers / total_users))\n",
    "   # print('\\tAvg # of retweets / tweet = %.3f' % (total_retweets / total_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_matrix(raw_df, index='date', mat_type='full', get_stats=False):\n",
    "    \"\"\"\n",
    "    Iterates through the rows of the created dataframe and performs summing, maxes, and \n",
    "    creates time series\n",
    "    \"\"\"\n",
    "    raw_df = raw_df.set_index(index)\n",
    "    time_series = raw_df.groupby(pd.TimeGrouper(freq='60Min'))\n",
    "    \n",
    "    X = np.zeros((len(time_series), 8))\n",
    "    y = np.zeros((len(time_series), 1))\n",
    "    total_tweets = []\n",
    "    total_retweets = []\n",
    "    stats = None\n",
    "\n",
    "    for i, (time_interval, g) in enumerate(time_series):\n",
    "        \"\"\"\n",
    "        #get the date for sorting\n",
    "        date = datetime.fromtimestamp(tweet_data['firstpost_date'])\n",
    "        df.set_value(i, 'date', date)\n",
    "        df.set_value(i, 'total_tweets', 1)\n",
    "        df.set_value(i, 'total_retweets', tweet['metrics']['citations']['total'])\n",
    "        \n",
    "        #will sum and take max in post processing\n",
    "        df.set_value(i, 'sum_followers', tweet['author']['followers'])\n",
    "        df.set_value(i, 'max_followers', tweet['author']['followers'])\n",
    "        df.set_value(i, 'total_replies', tweet['metrics']['citations']['replies'])\n",
    "        df.set_value(i, 'total_ranking', tweet['metrics']['ranking_score'])\n",
    "        df.set_value(i, 'total_impressions', tweet['metrics']['impressions'])\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        if(mat_type is 'full'):\n",
    "            X[i, 0] = g.total_tweets.sum()\n",
    "            X[i, 1] = g.total_retweets.sum()\n",
    "            X[i, 2] = g.sum_followers.sum()\n",
    "            X[i, 3] = g.max_followers.max()\n",
    "            X[i, 4] = time_interval.hour     #store the hour of the day -> preserve order\n",
    "            X[i, 5] = g.total_replies.sum()\n",
    "            X[i, 6] = g.total_ranking.sum()\n",
    "            X[i, 7] = g.total_impressions.sum()\n",
    "        elif(mat_type is 'partial'):\n",
    "            X[i, 0] = g.total_tweets.sum()\n",
    "            X[i, 1] = g.total_retweets.sum()\n",
    "            X[i, 2] = g.sum_followers.sum()\n",
    "            X[i, 3] = g.max_followers.max()\n",
    "            X[i, 4] = time_interval.hour     #store the hour of the day -> preserve order\n",
    "\n",
    "        if get_stats:\n",
    "            total_tweets.append(X[i,0])   #summed total tweets\n",
    "            total_retweets.append(X[i,1]) #summed total retweets\n",
    "            stats = (total_tweets, total_retweets)\n",
    "        \n",
    "    \n",
    "        \n",
    "        y[i, 0] = g.total_tweets.sum()\n",
    "        \n",
    "    return np.nan_to_num(X[:-1]), y[1:], stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 757/188136 [00:00<00:24, 7566.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Hashtag:  gohawks  with ntweets:  188136\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188136/188136 [00:19<00:00, 9834.69it/s]\n",
      "  3%|▎         | 876/26232 [00:00<00:02, 8748.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Hashtag:  gopatriots  with ntweets:  26232\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26232/26232 [00:02<00:00, 10106.58it/s]\n",
      "  0%|          | 0/259024 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Hashtag:  nfl  with ntweets:  259024\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 259024/259024 [00:26<00:00, 9738.93it/s]\n",
      "  0%|          | 0/489713 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Hashtag:  patriots  with ntweets:  489713\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 489713/489713 [00:47<00:00, 10296.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Hashtag:  sb49  with ntweets:  826951\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 826951/826951 [01:29<00:00, 9270.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Hashtag:  superbowl  with ntweets:  1348767\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1348767/1348767 [03:58<00:00, 5665.24it/s]\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "user_dicts = []\n",
    "for hashtag, num_tweets in hashtag_dict.items():\n",
    "    print(\"---\")\n",
    "    print(\"Hashtag: \", hashtag, \" with ntweets: \", num_tweets)\n",
    "    print(\"---\")\n",
    "    data[hashtag], user_dict = load_data(filename=os.path.join('tweet_data','tweets_#' + hashtag +'.txt'), num_tweets=num_tweets)\n",
    "    user_dicts.append(user_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = [data['gohawks'], \n",
    "          data['gopatriots'], \n",
    "          data['nfl'], \n",
    "          data['patriots'], \n",
    "          data['sb49'], \n",
    "          data['superbowl']]\n",
    "\n",
    "frame_hashtags=['gohawks', 'gopatriots', 'nfl', 'patriots', 'sb49', 'superbowl']\n",
    "\n",
    "\n",
    "all_data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#gohawks\n",
      "\tAvg # of tweets / hour = 193.357\n",
      "\tAvg # of followers / user = 1596.444\n",
      "\tAvg # of retweets / tweet = 2.015\n",
      "#gopatriots\n",
      "\tAvg # of tweets / hour = 38.351\n",
      "\tAvg # of followers / user = 1292.203\n",
      "\tAvg # of retweets / tweet = 1.400\n",
      "#nfl\n",
      "\tAvg # of tweets / hour = 279.422\n",
      "\tAvg # of followers / user = 4394.254\n",
      "\tAvg # of retweets / tweet = 1.539\n",
      "#patriots\n",
      "\tAvg # of tweets / hour = 499.198\n",
      "\tAvg # of followers / user = 1607.441\n",
      "\tAvg # of retweets / tweet = 1.783\n",
      "#sb49\n",
      "\tAvg # of tweets / hour = 1418.441\n",
      "\tAvg # of followers / user = 2229.695\n",
      "\tAvg # of retweets / tweet = 2.511\n",
      "#superbowl\n",
      "\tAvg # of tweets / hour = 1399.136\n",
      "\tAvg # of followers / user = 3675.339\n",
      "\tAvg # of retweets / tweet = 2.388\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "plotting_data = [] #superbowl and nfl\n",
    "for df in frames:\n",
    "    X, y, stats = build_matrix(df, get_stats=True)\n",
    "    total_tweets = stats[0]\n",
    "    total_retweets = stats[1]\n",
    "    user_dict = user_dicts[i]\n",
    "    \n",
    "    if(frame_hashtags[i] == 'superbowl' or frame_hashtags[i] == 'nfl'):     \n",
    "        plotting_data.append(total_tweets)\n",
    "        \n",
    "    avg_followers = float(sum(user_dict.values()))/len(user_dict.keys())\n",
    "    avg_tweets = np.mean(total_tweets)\n",
    "    avg_retweets = np.mean(total_retweets)/avg_tweets\n",
    "    print('#%s' % frame_hashtags[i])\n",
    "    \n",
    "    print('\\tAvg # of tweets / hour = %.3f' % (avg_tweets))\n",
    "    print('\\tAvg # of followers / user = %.3f' % (avg_followers))\n",
    "    print('\\tAvg # of retweets / tweet = %.3f' % (avg_retweets))\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188136, 8)\n",
      "(26232, 8)\n",
      "(259024, 8)\n",
      "(489713, 8)\n",
      "(826951, 8)\n",
      "(1348767, 8)\n",
      "(3138823, 8)\n",
      "3138823\n"
     ]
    }
   ],
   "source": [
    "#Verify that the dataframes are the right size\n",
    "print(data['gohawks'].shape)\n",
    "print(data['gopatriots'].shape)\n",
    "print(data['nfl'].shape)\n",
    "print(data['patriots'].shape)\n",
    "print(data['sb49'].shape)\n",
    "print(data['superbowl'].shape)\n",
    "\n",
    "assert(data['gohawks'].shape[0] == hashtag_dict['gohawks'])\n",
    "assert(data['gopatriots'].shape[0] == hashtag_dict['gopatriots'])\n",
    "assert(data['nfl'].shape[0] == hashtag_dict['nfl'])\n",
    "assert(data['patriots'].shape[0] == hashtag_dict['patriots'])\n",
    "assert(data['sb49'].shape[0] == hashtag_dict['sb49'])\n",
    "assert(data['superbowl'].shape[0] == hashtag_dict['superbowl'])\n",
    "\n",
    "\n",
    "print(all_data.shape)\n",
    "s=0\n",
    "for hashtag, ntweets in hashtag_dict.items():\n",
    "   s+=ntweets\n",
    "\n",
    "print(s)\n",
    "\n",
    "assert(all_data.shape[0] == s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Here, we show histograms with 1-hour bins that show the number the tweets in hour over time for two hashtag groups, #SuperBowl and #NFL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f4572b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f41ddd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "hashtags_to_plot = ['superbowl', 'nfl']\n",
    "i=0\n",
    "for hashtag in hashtags_to_plot:\n",
    "    total_tweets = plotting_data[i]\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('# of tweets')\n",
    "    plt.title('Number of tweets per hour: #' + hashtag)\n",
    "    \n",
    "    x_range = range(len(total_tweets))\n",
    "    plt.bar(x_range, total_tweets)\n",
    "    plt.show()\n",
    "    i+=1 \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.2\n",
    "For each hashtag, we first fitted a linear regression model using the following five features to\n",
    "predict the number of tweets in the next hour, with features extracted from tweet data in\n",
    "the previous hour.\n",
    "The features we used are:\n",
    "* Number of tweets (hashtag of interest)\n",
    "* Total number of retweets (hashtag of interest)\n",
    "* Sum of the number of followers of the users posting the hashtag\n",
    "* Maximum number of followers of the users posting the hashtag\n",
    "* Time of the day (which could take 24 values that represent hours of the day with respect to a given time zone)\n",
    "\n",
    "For each model, we present the mean squared error and r2 score. Further, we analyzed the significance of each feature using the t-test and P-value, using a third-party statsmodels.api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Mean squared error = 566824.371\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.498\n",
      "Model:                            OLS   Adj. R-squared:                  0.495\n",
      "Method:                 Least Squares   F-statistic:                     191.6\n",
      "Date:                Sat, 17 Mar 2018   Prob (F-statistic):          7.78e-142\n",
      "Time:                        16:00:56   Log-Likelihood:                -7818.7\n",
      "No. Observations:                 972   AIC:                         1.565e+04\n",
      "Df Residuals:                     967   BIC:                         1.567e+04\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             1.2946      0.133      9.754      0.000       1.034       1.555\n",
      "x2            -0.1660      0.044     -3.801      0.000      -0.252      -0.080\n",
      "x3            -0.0002   6.48e-05     -2.676      0.008      -0.000   -4.62e-05\n",
      "x4          7.037e-05      0.000      0.584      0.559      -0.000       0.000\n",
      "x5             4.8688      1.892      2.573      0.010       1.155       8.583\n",
      "const               0          0        nan        nan           0           0\n",
      "x6                  0          0        nan        nan           0           0\n",
      "x7                  0          0        nan        nan           0           0\n",
      "==============================================================================\n",
      "Omnibus:                     1700.263   Durbin-Watson:                   2.209\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          3686767.798\n",
      "Skew:                          11.016   Prob(JB):                         0.00\n",
      "Kurtosis:                     303.908   Cond. No.                          inf\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is      0. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jonny/anaconda3/lib/python3.6/site-packages/statsmodels/regression/linear_model.py:1471: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.sqrt(eigvals[0]/eigvals[-1])\n",
      "/Users/Jonny/anaconda3/lib/python3.6/site-packages/statsmodels/base/model.py:1036: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return self.params / self.bse\n",
      "/Users/Jonny/anaconda3/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/Jonny/anaconda3/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/Jonny/anaconda3/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1818: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Mean squared error = 27345.569\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.660\n",
      "Model:                            OLS   Adj. R-squared:                  0.658\n",
      "Method:                 Least Squares   F-statistic:                     263.3\n",
      "Date:                Sat, 17 Mar 2018   Prob (F-statistic):          3.58e-156\n",
      "Time:                        16:00:56   Log-Likelihood:                -4458.0\n",
      "No. Observations:                 683   AIC:                             8926.\n",
      "Df Residuals:                     678   BIC:                             8949.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -0.9102      0.233     -3.913      0.000      -1.367      -0.454\n",
      "x2             1.8188      0.233      7.821      0.000       1.362       2.275\n",
      "x3            -0.0006      0.000     -2.801      0.005      -0.001      -0.000\n",
      "x4             0.0003      0.000      1.567      0.118   -7.64e-05       0.001\n",
      "x5            -0.0321      0.489     -0.066      0.948      -0.992       0.928\n",
      "const               0          0        nan        nan           0           0\n",
      "x6                  0          0        nan        nan           0           0\n",
      "x7                  0          0        nan        nan           0           0\n",
      "==============================================================================\n",
      "Omnibus:                      470.173   Durbin-Watson:                   2.080\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           337818.087\n",
      "Skew:                           1.678   Prob(JB):                         0.00\n",
      "Kurtosis:                     111.901   Cond. No.                          inf\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is      0. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\t Mean squared error = 217537.196\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.644\n",
      "Model:                            OLS   Adj. R-squared:                  0.642\n",
      "Method:                 Least Squares   F-statistic:                     332.8\n",
      "Date:                Sat, 17 Mar 2018   Prob (F-statistic):          1.55e-203\n",
      "Time:                        16:00:57   Log-Likelihood:                -7007.2\n",
      "No. Observations:                 926   AIC:                         1.402e+04\n",
      "Df Residuals:                     921   BIC:                         1.405e+04\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.9383      0.131      7.142      0.000       0.680       1.196\n",
      "x2            -0.2501      0.065     -3.834      0.000      -0.378      -0.122\n",
      "x3          5.818e-05   2.06e-05      2.820      0.005    1.77e-05    9.87e-05\n",
      "x4         -3.562e-05    2.8e-05     -1.271      0.204   -9.06e-05    1.94e-05\n",
      "x5             3.8717      1.285      3.013      0.003       1.350       6.394\n",
      "const               0          0        nan        nan           0           0\n",
      "x6                  0          0        nan        nan           0           0\n",
      "x7                  0          0        nan        nan           0           0\n",
      "==============================================================================\n",
      "Omnibus:                     1079.475   Durbin-Watson:                   2.292\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1415668.110\n",
      "Skew:                           4.735   Prob(JB):                         0.00\n",
      "Kurtosis:                     194.315   Cond. No.                          inf\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is      0. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\t Mean squared error = 3382551.212\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.719\n",
      "Model:                            OLS   Adj. R-squared:                  0.717\n",
      "Method:                 Least Squares   F-statistic:                     498.7\n",
      "Date:                Sat, 17 Mar 2018   Prob (F-statistic):          1.05e-265\n",
      "Time:                        16:00:58   Log-Likelihood:                -8758.3\n",
      "No. Observations:                 980   AIC:                         1.753e+04\n",
      "Df Residuals:                     975   BIC:                         1.755e+04\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             1.7781      0.080     22.181      0.000       1.621       1.935\n",
      "x2            -0.8579      0.067    -12.766      0.000      -0.990      -0.726\n",
      "x3             0.0002   2.19e-05      7.303      0.000       0.000       0.000\n",
      "x4         -7.153e-05    7.5e-05     -0.953      0.341      -0.000    7.57e-05\n",
      "x5             8.0908      4.665      1.734      0.083      -1.063      17.245\n",
      "const               0          0        nan        nan           0           0\n",
      "x6                  0          0        nan        nan           0           0\n",
      "x7                  0          0        nan        nan           0           0\n",
      "==============================================================================\n",
      "Omnibus:                     1877.512   Durbin-Watson:                   1.712\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          4320561.851\n",
      "Skew:                          13.564   Prob(JB):                         0.00\n",
      "Kurtosis:                     327.151   Cond. No.                          inf\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is      0. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\t Mean squared error = 20100620.749\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.807\n",
      "Model:                            OLS   Adj. R-squared:                  0.806\n",
      "Method:                 Least Squares   F-statistic:                     483.8\n",
      "Date:                Sat, 17 Mar 2018   Prob (F-statistic):          1.10e-203\n",
      "Time:                        16:01:00   Log-Likelihood:                -5719.6\n",
      "No. Observations:                 582   AIC:                         1.145e+04\n",
      "Df Residuals:                     577   BIC:                         1.147e+04\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             1.0904      0.099     10.999      0.000       0.896       1.285\n",
      "x2            -0.1175      0.091     -1.291      0.197      -0.296       0.061\n",
      "x3          3.653e-06   1.45e-05      0.253      0.801   -2.48e-05    3.21e-05\n",
      "x4             0.0001   4.77e-05      2.213      0.027    1.19e-05       0.000\n",
      "x5            -4.1655     15.345     -0.271      0.786     -34.303      25.972\n",
      "const               0          0        nan        nan           0           0\n",
      "x6                  0          0        nan        nan           0           0\n",
      "x7                  0          0        nan        nan           0           0\n",
      "==============================================================================\n",
      "Omnibus:                     1171.838   Durbin-Watson:                   1.661\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2101434.987\n",
      "Skew:                          14.391   Prob(JB):                         0.00\n",
      "Kurtosis:                     295.965   Cond. No.                          inf\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is      0. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Mean squared error = 42576118.849\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.786\n",
      "Model:                            OLS   Adj. R-squared:                  0.785\n",
      "Method:                 Least Squares   F-statistic:                     703.7\n",
      "Date:                Sat, 17 Mar 2018   Prob (F-statistic):          1.04e-317\n",
      "Time:                        16:01:01   Log-Likelihood:                -9824.9\n",
      "No. Observations:                 963   AIC:                         1.966e+04\n",
      "Df Residuals:                     958   BIC:                         1.968e+04\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             1.3299      0.236      5.633      0.000       0.867       1.793\n",
      "x2             0.4026      0.117      3.434      0.001       0.173       0.633\n",
      "x3            -0.0002    1.2e-05    -20.299      0.000      -0.000      -0.000\n",
      "x4             0.0011   9.85e-05     11.032      0.000       0.001       0.001\n",
      "x5           -24.3155     17.329     -1.403      0.161     -58.322       9.691\n",
      "const               0          0        nan        nan           0           0\n",
      "x6                  0          0        nan        nan           0           0\n",
      "x7                  0          0        nan        nan           0           0\n",
      "==============================================================================\n",
      "Omnibus:                     1747.162   Durbin-Watson:                   2.235\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          4485126.325\n",
      "Skew:                          11.878   Prob(JB):                         0.00\n",
      "Kurtosis:                     336.488   Cond. No.                          inf\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is      0. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import datetime, time\n",
    "import pytz\n",
    "from itertools import compress\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_squared_error\n",
    "\n",
    "pst_tz = pytz.timezone('US/Pacific')\n",
    "\n",
    "for df in frames:\n",
    "    X, y, _ = build_matrix(df, mat_type='partial')\n",
    "    lr = linear_model.LinearRegression()\n",
    "    lr.fit(X, y)\n",
    "    y_pred = lr.predict(X)\n",
    "    print(\"\\t Mean squared error = %.3f\" % (mean_squared_error(y, y_pred)))\n",
    "    print(sm.OLS(y, X).fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From observing the significance, or p-value, of each feature, it is evident that the total number of tweets for the current hour is not a statistically significant feature as the p-value is larger than 0.05 (assuming alpha at 0.05). Every other feature, however, shows a low p-value, indicating that it contributes greatly our final prediction, and are therefore are valuable features. In general, a low p-value is evidence for the change in the predictor's value to be directly correlated to the change in the response variable, which is what we desire.\n",
    "\n",
    "Inutitively, this makes sense as the total tweets for the current hour would not affect the the total tweets for the next hour. Tweets do not become frequent when there is large number of tweets prior, instead, they become frequent when the previous tweets is associated with a large number of retweets and followers, signaling traction and actual social impact (at least virutally). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.3\n",
    "Design a regression model using any features from the papers you find or other new features you may find useful for this problem. Fit your model on the data of each hashtag and report fitting accuracy and significance of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first features we attempted to add were total number of \"favorites\" per hour, total number of replies per hour, and total number of verified tweeters posting that hour. The motivation for looking at the number of verified tweeters was the hypothesis that a verified public figure would be able to influence the number of tweets having to do with a subject he or she tweeted about. Most verified accounts are celebrities. For example, it's likely that if Tom Brady tweeted \"#gopatriots\", there would be more retweets and comments sharing this hashtag due to the network effect. It turned out that in the dataset, not many users are verified and the p-value indicated that this feature was not important. The model with these features added did signficantly worse and exhibited an R2-value of 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Mean squared error = 556864.412\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.507\n",
      "Model:                            OLS   Adj. R-squared:                  0.502\n",
      "Method:                 Least Squares   F-statistic:                     123.7\n",
      "Date:                Sat, 17 Mar 2018   Prob (F-statistic):          3.31e-142\n",
      "Time:                        16:01:02   Log-Likelihood:                -7810.0\n",
      "No. Observations:                 972   AIC:                         1.564e+04\n",
      "Df Residuals:                     964   BIC:                         1.567e+04\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             7.6429      1.962      3.896      0.000       3.793      11.493\n",
      "x2            -0.2044      0.050     -4.071      0.000      -0.303      -0.106\n",
      "x3            -0.0003    9.5e-05     -3.451      0.001      -0.001      -0.000\n",
      "x4         -8.104e-05      0.000     -0.645      0.519      -0.000       0.000\n",
      "x5             3.6629      1.917      1.911      0.056      -0.099       7.425\n",
      "x6            -6.1694      7.168     -0.861      0.390     -20.237       7.898\n",
      "x7            -1.3654      0.424     -3.223      0.001      -2.197      -0.534\n",
      "x8             0.0002   7.52e-05      2.627      0.009    4.99e-05       0.000\n",
      "==============================================================================\n",
      "Omnibus:                     1827.334   Durbin-Watson:                   2.214\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          4851359.955\n",
      "Skew:                          12.893   Prob(JB):                         0.00\n",
      "Kurtosis:                     348.140   Cond. No.                     9.36e+05\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 9.36e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\t Mean squared error = 24981.940\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.689\n",
      "Model:                            OLS   Adj. R-squared:                  0.686\n",
      "Method:                 Least Squares   F-statistic:                     187.3\n",
      "Date:                Sat, 17 Mar 2018   Prob (F-statistic):          8.76e-166\n",
      "Time:                        16:01:03   Log-Likelihood:                -4427.2\n",
      "No. Observations:                 683   AIC:                             8870.\n",
      "Df Residuals:                     675   BIC:                             8907.\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1           -13.8469      1.804     -7.676      0.000     -17.389     -10.305\n",
      "x2             2.4871      0.240     10.352      0.000       2.015       2.959\n",
      "x3            -0.0016      0.000     -3.782      0.000      -0.002      -0.001\n",
      "x4             0.0008      0.000      3.181      0.002       0.000       0.001\n",
      "x5            -0.0682      0.471     -0.145      0.885      -0.992       0.856\n",
      "x6            -8.6841      3.681     -2.359      0.019     -15.911      -1.457\n",
      "x7             2.6665      0.368      7.248      0.000       1.944       3.389\n",
      "x8             0.0005      0.000      1.696      0.090   -7.67e-05       0.001\n",
      "==============================================================================\n",
      "Omnibus:                      760.450   Durbin-Watson:                   1.889\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           306948.952\n",
      "Skew:                           4.550   Prob(JB):                         0.00\n",
      "Kurtosis:                     106.456   Cond. No.                     3.28e+05\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.28e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\t Mean squared error = 213628.897\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.650\n",
      "Model:                            OLS   Adj. R-squared:                  0.647\n",
      "Method:                 Least Squares   F-statistic:                     213.5\n",
      "Date:                Sat, 17 Mar 2018   Prob (F-statistic):          1.33e-203\n",
      "Time:                        16:01:04   Log-Likelihood:                -6998.3\n",
      "No. Observations:                 926   AIC:                         1.401e+04\n",
      "Df Residuals:                     918   BIC:                         1.405e+04\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             5.9031      1.296      4.554      0.000       3.359       8.447\n",
      "x2            -0.2009      0.066     -3.052      0.002      -0.330      -0.072\n",
      "x3          4.882e-05    3.4e-05      1.437      0.151   -1.79e-05       0.000\n",
      "x4          -2.58e-05   2.81e-05     -0.920      0.358   -8.09e-05    2.92e-05\n",
      "x5             3.1047      1.307      2.376      0.018       0.540       5.670\n",
      "x6            -5.0117      3.185     -1.574      0.116     -11.262       1.239\n",
      "x7            -1.1149      0.289     -3.853      0.000      -1.683      -0.547\n",
      "x8          7.719e-06   2.59e-05      0.298      0.766   -4.32e-05    5.86e-05\n",
      "==============================================================================\n",
      "Omnibus:                     1115.581   Durbin-Watson:                   2.340\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1327222.741\n",
      "Skew:                           5.094   Prob(JB):                         0.00\n",
      "Kurtosis:                     188.189   Cond. No.                     1.10e+06\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.1e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\t Mean squared error = 3303997.725\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.726\n",
      "Model:                            OLS   Adj. R-squared:                  0.723\n",
      "Method:                 Least Squares   F-statistic:                     321.1\n",
      "Date:                Sat, 17 Mar 2018   Prob (F-statistic):          9.87e-267\n",
      "Time:                        16:01:05   Log-Likelihood:                -8746.7\n",
      "No. Observations:                 980   AIC:                         1.751e+04\n",
      "Df Residuals:                     972   BIC:                         1.755e+04\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             7.7411      1.266      6.115      0.000       5.257      10.226\n",
      "x2            -0.4992      0.102     -4.911      0.000      -0.699      -0.300\n",
      "x3             0.0005      0.000      3.260      0.001       0.000       0.001\n",
      "x4            -0.0003   9.34e-05     -3.611      0.000      -0.001      -0.000\n",
      "x5             7.4431      4.646      1.602      0.109      -1.675      16.561\n",
      "x6             2.7513      4.536      0.606      0.544      -6.151      11.654\n",
      "x7            -1.5844      0.336     -4.722      0.000      -2.243      -0.926\n",
      "x8            -0.0002      0.000     -1.228      0.220      -0.000       0.000\n",
      "==============================================================================\n",
      "Omnibus:                     1905.666   Durbin-Watson:                   1.736\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          4520247.405\n",
      "Skew:                          14.030   Prob(JB):                         0.00\n",
      "Kurtosis:                     334.531   Cond. No.                     8.11e+05\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 8.11e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Mean squared error = 18516861.767\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.822\n",
      "Model:                            OLS   Adj. R-squared:                  0.820\n",
      "Method:                 Least Squares   F-statistic:                     332.2\n",
      "Date:                Sat, 17 Mar 2018   Prob (F-statistic):          8.72e-210\n",
      "Time:                        16:01:07   Log-Likelihood:                -5696.0\n",
      "No. Observations:                 582   AIC:                         1.141e+04\n",
      "Df Residuals:                     574   BIC:                         1.144e+04\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            12.9692      1.791      7.242      0.000       9.452      16.487\n",
      "x2             0.3982      0.116      3.423      0.001       0.170       0.627\n",
      "x3             0.0002    8.1e-05      2.049      0.041    6.91e-06       0.000\n",
      "x4          4.949e-05   5.93e-05      0.834      0.404    -6.7e-05       0.000\n",
      "x5             1.6699     15.047      0.111      0.912     -27.885      31.225\n",
      "x6           -13.8154      7.968     -1.734      0.083     -29.465       1.834\n",
      "x7            -3.0765      0.462     -6.658      0.000      -3.984      -2.169\n",
      "x8            -0.0001   8.06e-05     -1.625      0.105      -0.000    2.74e-05\n",
      "==============================================================================\n",
      "Omnibus:                     1145.750   Durbin-Watson:                   1.728\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2135236.482\n",
      "Skew:                          13.625   Prob(JB):                         0.00\n",
      "Kurtosis:                     298.480   Cond. No.                     1.04e+07\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.04e+07. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\t Mean squared error = 40498445.505\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.796\n",
      "Model:                            OLS   Adj. R-squared:                  0.795\n",
      "Method:                 Least Squares   F-statistic:                     467.1\n",
      "Date:                Sat, 17 Mar 2018   Prob (F-statistic):          4.94e-324\n",
      "Time:                        16:01:09   Log-Likelihood:                -9800.8\n",
      "No. Observations:                 963   AIC:                         1.962e+04\n",
      "Df Residuals:                     955   BIC:                         1.966e+04\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1           -11.7704      2.820     -4.173      0.000     -17.305      -6.235\n",
      "x2             0.6362      0.121      5.256      0.000       0.399       0.874\n",
      "x3             0.0008      0.000      3.732      0.000       0.000       0.001\n",
      "x4             0.0010      0.000      9.921      0.000       0.001       0.001\n",
      "x5           -11.8030     17.077     -0.691      0.490     -45.315      21.709\n",
      "x6            58.3133     17.813      3.274      0.001      23.356      93.270\n",
      "x7             2.7623      0.600      4.603      0.000       1.585       3.940\n",
      "x8            -0.0011      0.000     -4.951      0.000      -0.002      -0.001\n",
      "==============================================================================\n",
      "Omnibus:                     1920.101   Durbin-Watson:                   2.088\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          5295072.478\n",
      "Skew:                          14.773   Prob(JB):                         0.00\n",
      "Kurtosis:                     365.066   Cond. No.                     1.16e+07\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.16e+07. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import datetime, time\n",
    "import pytz\n",
    "from itertools import compress\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_squared_error\n",
    "\n",
    "pst_tz = pytz.timezone('US/Pacific')\n",
    "\n",
    "for df in frames:\n",
    "    X, y,_ = build_matrix(df, mat_type='full')\n",
    "    lr = linear_model.LinearRegression()\n",
    "    lr.fit(X, y)\n",
    "    y_pred = lr.predict(X)\n",
    "    print(\"\\t Mean squared error = %.3f\" % (mean_squared_error(y, y_pred)))\n",
    "    print(sm.OLS(y, X).fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we initially started with the following features:\n",
    "- total number of favorites per hour\n",
    "- total number of replies per hour\n",
    "- total number of verified tweeters that hour\n",
    "\n",
    "Favorites and verified tweets were ineffective features. There are very few verified accounts so it resulted in sparsity and not much information gain. The number of favorites was also pretty low. The total number of replies seemed to improve performance, so we left it in. The next test involved the following three features, after having removed favorites and verified tweeters:\n",
    "- total number of replies per hour\n",
    "- total ranking \n",
    "- total impressions\n",
    "\n",
    "Ranking and impressions indicate the popularity or visibility of a tweet/tweeter, so these seemed like reasonable additions. These features improved RMSE performance by several points in terms of R2, and were vastly more effective than the previously added sparse features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.4\n",
    "In this part, we would like to perform 10-fold cross-validation on the models from the previous part and calculate the average prediction error over samples in the held-out part for the 10 tests. For this problem, you should split the feature data (your set of (features, predictant) pairs for windows) into 10 parts to perform cross-validation. Also, your evaluated error should be of the form |Npredicted − Nreal|.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error #RMSE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "def cv(X, y, model, n_splits=10, verbose=True, display_last_ols=False):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    rmses = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        y_train = y_train.ravel()\n",
    "        y_test = y_test.ravel()\n",
    "        #print(sm.OLS(y, X).fit().summary())\n",
    "#        print(X_train.shape)\n",
    "#        print(y_train.shape)\n",
    "        if(model == 'lr'):\n",
    "            lr = sm.OLS(y_train, X_train).fit()\n",
    "            y_preds = lr.predict(X_test)\n",
    "        elif(model == 'rf'):\n",
    "            rf = RandomForestRegressor()\n",
    "            rf.fit(X_train, y_train)\n",
    "            y_preds = rf.predict(X_test)\n",
    "        elif(model == 'mlp'):\n",
    "            mlp = MLPRegressor()\n",
    "            mlp.fit(X_train, y_train)\n",
    "            y_preds = mlp.predict(X_test)\n",
    "        \n",
    "        rmses.append(mean_squared_error(y_test, y_preds))\n",
    "        \n",
    "        \n",
    "    if verbose: \n",
    "        print(\"Errors from CV are: \", rmses)\n",
    "        print(\"Averaged error is: \", np.mean(rmses))\n",
    "        if model is 'lr':\n",
    "            print(lr.summary())\n",
    "        \n",
    "    return rmses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#datetime args: Attributes: year, month, day, hour, minute, second, microsecond, and tzinfo.\n",
    "#before Feb 1, 8:00am\n",
    "first_date_marker = datetime.datetime(2015, 2, 1, 8, 0, 0, 0)\n",
    "\n",
    "#end at 8pm\n",
    "second_date_marker = datetime.datetime(2015, 2, 1, 20, 0, 0, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_and_test(df, model, verbose=True):\n",
    "    ###Set up the data by filtering via index\n",
    "    #Before Feb. 1, 8:00 a.m.\n",
    "    #sort out the times in the dataframe before this period\n",
    "    df_p1 = df[df.date < first_date_marker]\n",
    "\n",
    "\n",
    "    #Between Feb. 1, 8:00 a.m. and 8:00 p.m. \n",
    "    df_p2 = df[(df.date > first_date_marker) &\n",
    "               (df.date < second_date_marker)]\n",
    "\n",
    "    #After Feb. 1, 8:00 p.m.\n",
    "    df_p3 = df[df.date > second_date_marker]\n",
    "\n",
    "    print(\"Before Feb. 1, 8:00 a.m.\")\n",
    "    X_df_p1, y_df_p1, _ = build_matrix(df_p1, index='date')\n",
    "    errors_df_p1 = cv(X_df_p1, y_df_p1, model, verbose=verbose) #default splits = 10 no need to specify\n",
    "   # print(\"Average cv error: \", np.mean(errors_df_p1))\n",
    "    \n",
    "    print(\"Between Feb. 1, 8:00 a.m. and 8:00 p.m.\")\n",
    "    X_df_p2, y_df_p2, _ = build_matrix(df_p2, index='date')\n",
    "    errors_df_p2 = cv(X_df_p2, y_df_p2, model,verbose=verbose)\n",
    "  #  print(\"Average cv error: \", np.mean(errors_df_p2))\n",
    "    \n",
    "    print(\"After Feb. 1, 8:00 p.m.\")\n",
    "    X_df_p3, y_df_p3, _ = build_matrix(df_p3, index='date')\n",
    "    errors_df_p3 = cv(X_df_p3, y_df_p3, model,verbose=verbose)\n",
    "   # print(\"Average cv error: \", np.mean(errors_df_p3))\n",
    "\n",
    "    return np.mean(errors_df_p1), np.mean(errors_df_p2), np.mean(errors_df_p3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Hashtag Performance with 3 Models\n",
    "For each hashtag, report the average cross-validation errors for the 3 different models. Note that you should do the 90-10% splitting for each model within its specific time window. I.e. Only use data within one of the 3 periods above for training and testing each time, so for each period you will run 10 tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%javascript\n",
    "#IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "#    return false;\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is: lr\n",
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m.\n",
      "Hashtag:  gohawks\n",
      "Period 1 avg. cv error:  1382106.04845\n",
      "Period 2 avg. cv error:  5101080553.83\n",
      "Period 3 avg. cv error:  340217.549689\n",
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m.\n",
      "Hashtag:  gopatriots\n",
      "Period 1 avg. cv error:  2403.64668795\n",
      "Period 2 avg. cv error:  75715827423.6\n",
      "Period 3 avg. cv error:  80.8086140564\n",
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m.\n",
      "Hashtag:  nfl\n",
      "Period 1 avg. cv error:  58859.8755465\n",
      "Period 2 avg. cv error:  315472851.69\n",
      "Period 3 avg. cv error:  25227.5050623\n",
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m.\n",
      "Hashtag:  patriots\n",
      "Period 1 avg. cv error:  368516.190929\n",
      "Period 2 avg. cv error:  9815441030.44\n",
      "Period 3 avg. cv error:  51652.4952138\n",
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m.\n",
      "Hashtag:  sb49\n",
      "Period 1 avg. cv error:  11563.2334898\n",
      "Period 2 avg. cv error:  18321600567.0\n",
      "Period 3 avg. cv error:  120403.776686\n",
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m.\n",
      "Hashtag:  superbowl\n",
      "Period 1 avg. cv error:  417466.104268\n",
      "Period 2 avg. cv error:  51943860578.5\n",
      "Period 3 avg. cv error:  354705.82262\n",
      "Model is: rf\n",
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m.\n",
      "Hashtag:  gohawks\n",
      "Period 1 avg. cv error:  435420.589467\n",
      "Period 2 avg. cv error:  12924538.54\n",
      "Period 3 avg. cv error:  6386.98540069\n",
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m.\n",
      "Hashtag:  gopatriots\n",
      "Period 1 avg. cv error:  2173.01734416\n",
      "Period 2 avg. cv error:  804967.996\n",
      "Period 3 avg. cv error:  114.156900345\n",
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m.\n",
      "Hashtag:  nfl\n",
      "Period 1 avg. cv error:  45248.4095179\n",
      "Period 2 avg. cv error:  8964833.8795\n",
      "Period 3 avg. cv error:  41227.8766868\n",
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m.\n",
      "Hashtag:  patriots\n",
      "Period 1 avg. cv error:  362194.018316\n",
      "Period 2 avg. cv error:  422717974.397\n",
      "Period 3 avg. cv error:  67720.3168077\n",
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m.\n",
      "Hashtag:  sb49\n",
      "Period 1 avg. cv error:  18963.7523944\n",
      "Period 2 avg. cv error:  1192551636.33\n",
      "Period 3 avg. cv error:  103688.102929\n",
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m.\n",
      "Hashtag:  superbowl\n",
      "Period 1 avg. cv error:  434499.050771\n",
      "Period 2 avg. cv error:  5347358576.95\n",
      "Period 3 avg. cv error:  301257.262011\n",
      "Model is: mlp\n",
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m.\n",
      "Hashtag:  gohawks\n",
      "Period 1 avg. cv error:  377919453.472\n",
      "Period 2 avg. cv error:  441797516704.0\n",
      "Period 3 avg. cv error:  7712473816.71\n",
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m.\n",
      "Hashtag:  gopatriots\n",
      "Period 1 avg. cv error:  153289026.571\n",
      "Period 2 avg. cv error:  8155155499.35\n",
      "Period 3 avg. cv error:  233188.409009\n",
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m.\n",
      "Hashtag:  nfl\n",
      "Period 1 avg. cv error:  7662972716.11\n",
      "Period 2 avg. cv error:  585733350238.0\n",
      "Period 3 avg. cv error:  20491538042.2\n",
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m.\n",
      "Hashtag:  patriots\n",
      "Period 1 avg. cv error:  6150921900.55\n",
      "Period 2 avg. cv error:  5.32330786512e+12\n",
      "Period 3 avg. cv error:  9825216538.81\n",
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m.\n",
      "Hashtag:  sb49\n",
      "Period 1 avg. cv error:  187357900426.0\n",
      "Period 2 avg. cv error:  4.21888355865e+14\n",
      "Period 3 avg. cv error:  805161747558.0\n",
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m.\n",
      "Hashtag:  superbowl\n",
      "Period 1 avg. cv error:  145026561.811\n",
      "Period 2 avg. cv error:  5.24521342848e+14\n",
      "Period 3 avg. cv error:  1.02530075757e+12\n",
      "CPU times: user 38.1 s, sys: 2.09 s, total: 40.2 s\n",
      "Wall time: 40.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "models = ['lr', 'rf', 'mlp']\n",
    "results = {}\n",
    "for model in models:\n",
    "    print(\"Model is: \"+model)\n",
    "    i = 0\n",
    "    p1_errs, p2_errs, p3_errs = [],[],[]\n",
    "    for df in frames:\n",
    "        err_p1, err_p2, err_p3 = filter_and_test(df, model, verbose=False)\n",
    "        print(\"Hashtag: \", frame_hashtags[i])\n",
    "        print(\"Period 1 avg. cv error: \", err_p1)\n",
    "        print(\"Period 2 avg. cv error: \", err_p2)\n",
    "        print(\"Period 3 avg. cv error: \", err_p3)\n",
    "        p1_errs.append(err_p1)\n",
    "        p2_errs.append(err_p2)\n",
    "        p3_errs.append(err_p3)\n",
    "        i+=1\n",
    "    results[model] = [np.mean(p1_errs), np.mean(p2_errs), np.mean(p3_errs)]\n",
    "            \n",
    "#            full_errors = cv(X_full, y_full, n_splits=10)\n",
    " #           print(\"Errors from full set CV: \", full_errors)\n",
    "  #          print(\"Mean error from CV: \", np.mean(full_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26869402701.343662, 1164523570.2715075, 159139838766154.69]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "sum_errors = []\n",
    "for model, result_arr in results.items():\n",
    "    sum_errors.append(np.sum(result_arr))\n",
    "    \n",
    "print(sum_errors)    \n",
    "print(np.argmin(sum_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate models holistically, we first run each model on each period independently. The period-specific cross-validation errors are stored in three lists. Then at the end, the list values are averaged across all hashtags. So each model will have three corresponding error values associated with it corresponding to the averaged cross-validation error for each time period (1-3). To select the best performing model overall, we iterated through the model:error_list pairs and summed the three errors in error_list. Then, we chose the model with the lowewst summed value. We found that the random forest model with sklearn's default configuration performed better than linear regression or an MLP. Thus, the random forest model will be used for the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated Data of all Hashtags\n",
    "\n",
    "In the previous step, data of all hashtags was aggregated. The function call below only means something when more than one hashtag is involved, otherwise results will be the same as the previous time interval split, since the dataframe will only contain data from one set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m.\n"
     ]
    }
   ],
   "source": [
    "err_p1, err_p2, err_p3 = filter_and_test(all_data, model='rf', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period 1 avg. cv error:  3290069.35568\n",
      "Period 2 avg. cv error:  12174800333.2\n",
      "Period 3 avg. cv error:  488201.588797\n"
     ]
    }
   ],
   "source": [
    "print(\"Period 1 avg. cv error: \", err_p1)\n",
    "print(\"Period 2 avg. cv error: \", err_p2)\n",
    "print(\"Period 3 avg. cv error: \", err_p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[216416.47296843663, 1164220421.3484166, 86732.450122516966]\n"
     ]
    }
   ],
   "source": [
    "print(results['rf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.5\n",
    "Download the test data3. Each file in the test data contains a hashtag’s tweets for a 6-hour window (note that these hashtags are different from those in training data). Fit a model on the aggregate of the training data for all hashtags, and predict the number of tweets in the next hour for each test file. The file names show sample number followed by the period number the data is from. E.g. a file named sample5 period2.txt contains tweets for a 6-hour window that lies in the 2nd time period described in part 4. One can be creative here, and use the data from all previous 6 hours for making more accurate predictions (as opposed to using features from the previous hour only).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Report the model you use. For each test file, provide your predictions on the number of tweets in the next hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dict = {\n",
    "    \"sample1_period1.txt\":730,\n",
    "    \"sample2_period2.txt\":212273,\n",
    "    \"sample3_period3.txt\":3628,\n",
    "    \"sample4_period1.txt\":1646,\n",
    "    \"sample5_period1.txt\":2059,\n",
    "    \"sample6_period2.txt\":205554,\n",
    "    \"sample7_period3.txt\":528,\n",
    "    \"sample8_period1.txt\":229,\n",
    "    \"sample9_period2.txt\":11311,\n",
    "    \"sample10_period3.txt\":365\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_string_info(s):\n",
    "    s = s[:-4]\n",
    "    samp, per = s.split(\"_\")\n",
    "    return int(samp[-1:]), int(per[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From piazza: If you are using 'first post date' as date, just train a single model across all the training data for different periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 730/730 [00:00<00:00, 8320.70it/s]\n",
      "100%|██████████| 212273/212273 [00:18<00:00, 11183.06it/s]\n",
      "100%|██████████| 3628/3628 [00:00<00:00, 8392.09it/s]\n",
      "100%|██████████| 1646/1646 [00:00<00:00, 10410.54it/s]\n",
      "100%|██████████| 2059/2059 [00:00<00:00, 10958.30it/s]\n",
      "100%|██████████| 205554/205554 [00:21<00:00, 9696.53it/s] \n",
      "100%|██████████| 528/528 [00:00<00:00, 6044.72it/s]\n",
      "100%|██████████| 229/229 [00:00<00:00, 7215.75it/s]\n",
      "100%|██████████| 11311/11311 [00:01<00:00, 10817.98it/s]\n",
      "100%|██████████| 365/365 [00:00<00:00, 9054.79it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "error_dict={}\n",
    "pred_dict={}\n",
    "\n",
    "X_all, y_all, _ = build_matrix(all_data, index='date')\n",
    "y_all = y_all.ravel()\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_all, y_all)\n",
    "\n",
    "for filename, num_items in test_dict.items():\n",
    "#    print(filename, num_items)\n",
    "    full_path = 'test_data/'+filename\n",
    "    df, _ = load_data(filename=full_path, num_tweets=num_items)\n",
    "    X_test, y_test, _ = build_matrix(df)\n",
    "    y_test = y_test.ravel()\n",
    "    \n",
    "    y_preds = rf.predict(X_test)\n",
    "    pred_dict[filename] = y_preds\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_preds)\n",
    "    \n",
    "    error_dict[filename] = mae\n",
    "    #we want to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sample1_period1.txt': 62.759999999999991, 'sample2_period2.txt': 52135.520000000004, 'sample3_period3.txt': 225.59999999999999, 'sample4_period1.txt': 163.97999999999999, 'sample5_period1.txt': 81.879999999999995, 'sample6_period2.txt': 26404.799999999999, 'sample7_period3.txt': 55.579999999999998, 'sample8_period1.txt': 124.07499999999999, 'sample9_period2.txt': 398.77999999999997, 'sample10_period3.txt': 106.13999999999999}\n",
      "sample1_period1.txt [ 223.9  146.4  139.5  142.4  158.6]\n",
      "sample2_period2.txt [  11876.2   12892.6   11982.7   13428.   261953.5]\n",
      "sample3_period3.txt [  737.6   538.2   994.9  1074.    826.7]\n",
      "sample4_period1.txt [ 808.2  308.   295.6  307.2  327.9]\n",
      "sample5_period1.txt [ 596.   416.   442.   376.   296.4]\n",
      "sample6_period2.txt [   939.6  25615.3  81737.9  80610.   20908. ]\n",
      "sample7_period3.txt [ 134.2  129.5  136.   143.2  138. ]\n",
      "sample8_period1.txt [ 129.5  149.8  217.6  179.4]\n",
      "sample9_period2.txt [ 2136.6  2061.6  2055.1  2031.2  2288.6]\n",
      "sample10_period3.txt [ 149.   217.6  165.2  148.2  153.7]\n"
     ]
    }
   ],
   "source": [
    "print(error_dict)\n",
    "\n",
    "for filename, preds in pred_dict.items():\n",
    "    print(filename, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fan Base Predicition\n",
    "\n",
    "In this next section, we trained three different types of binary classification models to predict the location of the tweet's user based on the pure semantics of their tweets. For example, a user from Washington will most likely root for their local Redskins and will produce a positive sentiment in their tweet. We exploit this, and thus train our models. As a preprocessing step, we converted the tweet's text into numerical features by applying a TF-IDF based vectorizer (setting min_df = 3). The dataset was split into training and test set, in a 75:25 ratio.\n",
    "\n",
    "a) First, we employed a basic logistic regression model, and the results are as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/TOCOMBINE/tweet_data/tweets_#gopatriots.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-6f78c0fa5bc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# this takes awhile so use toy example of #gopatriots only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtweets_sb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/tweets_#gopatriots.txt'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/TOCOMBINE/tweet_data/tweets_#gopatriots.txt'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data_dir = '/Volumes/TOCOMBINE/tweet_data' # MAKE SURE TO CHANGE THIS TO WHERE EVER YOUR DATA IS. \n",
    "# The total data size is ~14gb which is too large to be committed into github\n",
    "\n",
    "# Puts data into dict\n",
    "# hashtags = ['gohawks', 'gopatriots', 'nfl', 'patriots', 'sb49', 'superbowl'] \n",
    "# this takes awhile so use toy example of #gopatriots only\n",
    "tweets_sb = []\n",
    "with open(data_dir + '/tweets_#gopatriots.txt' , 'r') as f:\n",
    "    for i, l in enumerate(f):\n",
    "        tweet = json.loads(l)\n",
    "        tweets_sb.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import re\n",
    "\n",
    "class MyTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.token_pattern = re.compile(u'(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(w) for w in self.token_pattern.findall(doc)]\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=3, stop_words=text.ENGLISH_STOP_WORDS, tokenizer=MyTokenizer()) \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "washington_terms = ['Seattle, Washington', 'Washington', 'WA', 'Seattle, WA', 'Kirkland, Washington']\n",
    "massachusetts_terms = ['massachusetts', 'ma']\n",
    "\n",
    "def find_location(loc):\n",
    "    for wt in washington_terms:\n",
    "        if loc.lower().find(wt.lower()) >= 0:\n",
    "            return \"washington\" # 0\n",
    "    for mt in massachusetts_terms:\n",
    "        if loc.lower().find(mt.lower()) >= 0:\n",
    "            return \"massachusetts\" # 1\n",
    "    return None\n",
    "\n",
    "X, y = [], []\n",
    "for tweet in tweets_sb:\n",
    "    location = find_location(tweet['tweet']['user']['location'])\n",
    "    if location is None: continue # does not belong to either state\n",
    "    X.append(tweet['tweet']['text'])\n",
    "    y.append(0 if location == \"washington\" else 1)\n",
    "\n",
    "X = vectorizer.fit_transform(X)\n",
    "X = tfidf_transformer.fit_transform(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "# print(X.shape, y.shape)\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X_train, y_train)\n",
    "y_pred = logistic.predict(X_test)\n",
    "y_pred_prob = logistic.predict_proba(X_test)\n",
    "show_stats(y_test, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_stats(y_test, y_pred, y_pred_prob):\n",
    "    import itertools\n",
    "    from sklearn import linear_model\n",
    "    from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, precision_score, accuracy_score, recall_score\n",
    "\n",
    "    def plot_confusion_matrix(cm, classes,\n",
    "                              normalize=False,\n",
    "                              title='Confusion matrix',\n",
    "                              cmap=plt.cm.Blues):\n",
    "        \"\"\"\n",
    "        This function prints and plots the confusion matrix.\n",
    "        Normalization can be applied by setting `normalize=True`.\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "        fmt = '.2f' if normalize else 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()\n",
    "\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    plot_confusion_matrix(cnf_matrix, ['WA', 'MA'], title='Confusion Matrix')\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:,0], pos_label=0)\n",
    "    plt.plot(fpr, tpr, label='AUC =  %.3f' % roc_auc_score(y_test, y_pred_prob[:,0]))\n",
    "    plt.title('ROC Curve')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print('Accuracy = %.3f, Precision = %.3f, Recall = %.3f' % (accuracy_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Afterwards, we applied a more complex model of a Linear support vector machine. The results are as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel='linear', probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_prob = clf.predict_proba(X_test)\n",
    "show_stats(y_test, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Finally, we employed a multinomial Naive Bayes classifier. This type of model is suitable for classification with discrete features and in practice, fractional counts such as tf-idf representations also work, which is the case in our machine learning problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_prob = clf.predict_proba(X_test)\n",
    "show_stats(y_test, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, comparing these three models' results, one can claim that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Game Results based on Tweet Sentiment\n",
    "\n",
    "To build upon the previous section's exploration with using tweet semantics for prediction, we will describe a model that can predict the winner (and loser) of the game based on the current tweet sentiment for both participating teams.\n",
    "Given that team A and team B are paritipating in the superbowl, the features we used are:\n",
    "* Total number of positive tweets for #go{team A}\n",
    "* Total number of negative tweets for #go{team A}\n",
    "* Total number of retweets for #go{team A}\n",
    "* Total number of positive tweets for #go{team B}\n",
    "* Total number of negative tweets for #go{team B}\n",
    "* Total number of retweets for #go{team B}\n",
    "\n",
    "For our given dataset of superbowl 15, we collect the tweet statistics from #gopatriots and #gohawks. To determine whether a given tweet is positive, we use the sentiment analyzer API provided by Textblob (https://pypi.python.org/pypi/textblob)\n",
    "\n",
    "We employed a logistic regression model to predict the outcome of the game. Unfortunately, since our given dataset is regarding only one superbowl game, we could only create one training example, which of course is not sufficient to generate a robust model. If we had sufficient time, we would first collect the same kind of tweet statistics for all the past superbowl games (starting from superbowl 2007 since Twitter was founded in early 2006). In addition, since this gives us only ten more examples, we would collect tweet datasets for at least a couple hundred NFL football games to finally generate a decently sized training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/Volumes/TOCOMBINE/tweet_data' # MAKE SURE TO CHANGE THIS TO WHERE EVER YOUR DATA IS. \n",
    "# The total data size is ~14gb which is too large to be committed into github\n",
    "\n",
    "# Puts data into dict\n",
    "hashtags = ['gopatriots']\n",
    "data = {}\n",
    "for hashtag in hashtags:\n",
    "    file_name = data_dir + '/tweets_#' + hashtag + '.txt' \n",
    "    with open(file_name, 'r') as f:\n",
    "        tweets = []\n",
    "        for i, l in enumerate(f):\n",
    "            tweet = json.loads(l)\n",
    "            tweets.append(tweet)\n",
    "        data[hashtag] = tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import regex as re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Utility function to clean tweet text by removing links, special characters\n",
    "    using simple regex statements.\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "def get_tweet_sentiment(tweet):\n",
    "    '''\n",
    "    Utility function to classify sentiment of passed tweet\n",
    "    using textblob's sentiment method\n",
    "    '''\n",
    "    # create TextBlob object of passed tweet text\n",
    "    analysis = TextBlob(clean_tweet(tweet))\n",
    "    # set sentiment\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "row = []\n",
    "for hashtag in hashtags:\n",
    "    positives, negatives, retweets = 0, 0, 0\n",
    "    for tweet in data[hashtag]:\n",
    "        sentiment = get_tweet_sentiment(tweet['tweet']['text'])\n",
    "        if sentiment is 'positive':\n",
    "            positives += 1\n",
    "        elif sentiment is 'negative':\n",
    "            negatives += 1\n",
    "        retweets += tweet['metrics']['citations']['total']\n",
    "    row.append(positives)\n",
    "    row.append(negatives)\n",
    "    row.append(retweets)\n",
    "X = np.array(row)\n",
    "y = np.array([1,0]) # Patriots won, Seahwaks lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X, y)\n",
    "y_pred = logistic.predict(X)\n",
    "y_pred_prob = logistic.predict_proba(X)\n",
    "show_stats(y, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the logistic regression model is summarized above. This, of course, is meaningless to analyze as we only utilized one trainig example. However, with more data, we are certain that we can create a generalizable predictor of superbowl champions based on both teams' fans' tweet sentiment. Our intuition is that the winning team will gather more both positive and negative tweets than those of the losing team, as the winning fan is most likely to tweet a positive tweet about his or her winning team, and the losing fan is most likely to tweet a negative tweet about the opposing team's undeserved victory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
